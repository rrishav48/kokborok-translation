{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b763d5-60ce-4cf9-8428-2991efb581c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, LSTM, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdcab2c8-610d-4bf1-b002-73a1411be6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi:\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(\"kokdata2.csv\")\n",
    "print(\"hi:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3962af8b-d161-4086-afb7-7ce0a97bd759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        English      Kokborok  Eng_Count  kok_Count\n",
      "0         Hello        Khulum          1          1\n",
      "1  How are you?   Bwrwi ngai?          3          2\n",
      "2  Good morning  Bwrwi Swrang          2          2\n",
      "3     Thank you   Bathwngthai          2          1\n",
      "4           Yes            Ee          1          1\n"
     ]
    }
   ],
   "source": [
    "# def word_count(text):\n",
    "#     if isinstance(text, str):  # Ensure the value is a string\n",
    "#         return len(text.split())\n",
    "#     return 0  # If NaN, return 0\n",
    "\n",
    "# # Apply word count function\n",
    "# data['Eng_Count'] = data['English words/sentences'].apply(word_count)\n",
    "# data['kok_Count'] = data['Kok words/sentences'].apply(word_count)\n",
    "\n",
    "# # Display updated dataset\n",
    "# print(data.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def word_count(text):\n",
    "    if isinstance(text, str):  # Ensure the value is a string\n",
    "        return len(text.split())\n",
    "    return 0  # If NaN, return 0\n",
    "\n",
    "# Strip whitespace from columns\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Apply word count function\n",
    "data['Eng_Count'] = data['English'].apply(word_count)\n",
    "data['kok_Count'] = data['Kokborok'].apply(word_count)\n",
    "\n",
    "# Display updated dataset\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89361281-f76b-45db-ac7d-06fc380856a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiii\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ensure data is in string format\n",
    "data['Eng_Count'] = data['Eng_Count'].astype(str)\n",
    "data['kok_Count'] = data['kok_Count'].astype(str)  # Assuming 'kok_Count' is the correct column\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_and_pad(texts, num_words=10000, maxlen=20):\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(texts.astype(str))  # Convert Series to string format\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "    return tokenizer, padded_sequences\n",
    "\n",
    "# Apply tokenization\n",
    "eng_tokenizer, eng_sequences = tokenize_and_pad(data['Eng_Count'])\n",
    "frn_tokenizer, frn_sequences = tokenize_and_pad(data['kok_Count'])  # Check if 'kok_Count' is correct\n",
    "print(\"hiii\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9a9cda9-04ac-4811-84a7-1c7964c104ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(eng_sequences, frn_sequences, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e0f5ea-6069-4fdc-a657-bcbe8dd38ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "# Model creation\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64),  # Removed input_length\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    TimeDistributed(Dense(10000, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1416fa9-890a-4442-bc35-235ef7a1c4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11s/step - accuracy: 0.0000e+00 - loss: 9.2065 - val_accuracy: 0.9500 - val_loss: 9.1891\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 888ms/step - accuracy: 0.9500 - loss: 9.1892 - val_accuracy: 0.9500 - val_loss: 9.1698\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 566ms/step - accuracy: 0.9500 - loss: 9.1700 - val_accuracy: 0.9500 - val_loss: 9.1463\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 632ms/step - accuracy: 0.9500 - loss: 9.1464 - val_accuracy: 0.9500 - val_loss: 9.1160\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 555ms/step - accuracy: 0.9500 - loss: 9.1161 - val_accuracy: 0.9500 - val_loss: 9.0765\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 565ms/step - accuracy: 0.9500 - loss: 9.0767 - val_accuracy: 0.9500 - val_loss: 9.0252\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 643ms/step - accuracy: 0.9500 - loss: 9.0254 - val_accuracy: 0.9500 - val_loss: 8.9587\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 650ms/step - accuracy: 0.9500 - loss: 8.9590 - val_accuracy: 0.9500 - val_loss: 8.8735\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step - accuracy: 0.9500 - loss: 8.8739 - val_accuracy: 0.9500 - val_loss: 8.7660\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 642ms/step - accuracy: 0.9500 - loss: 8.7665 - val_accuracy: 0.9500 - val_loss: 8.6334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x185ad9337a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training\n",
    "checkpoint = ModelCheckpoint('model.keras', save_best_only=True)\n",
    "#checkpoint = ModelCheckpoint('model.h5', save_best_only=True)\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5edc29-c152-42d9-9ba8-0049c93da9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
